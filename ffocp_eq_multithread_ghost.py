import time
import cvxpy as cp
import numpy as np
import os

import torch
from cvxtorch import TorchExpression
from cvxpylayers.torch import CvxpyLayer
import wandb
from utils import to_numpy, to_torch, _dump_cvxpy, n_threads, slice_params_for_batch


from multiprocessing.pool import ThreadPool
from threadpoolctl import threadpool_limits
import multiprocessing as mp

@torch.no_grad()
def _compare_grads(params_req, grads, ground_truth_grads):
    est_chunks, gt_chunks = [], []
    for p, ge, gg in zip(params_req, grads, ground_truth_grads):
        ge = torch.zeros_like(p) if ge is None else ge.detach()
        gg = torch.zeros_like(p) if gg is None else gg.detach()
        est_chunks.append(ge.reshape(-1))
        gt_chunks.append(gg.reshape(-1))

    est = torch.cat(est_chunks)
    gt  = torch.cat(gt_chunks)

    eps = 1e-12

    denom = (est.norm() * gt.norm()).clamp_min(eps)
    cos_sim = torch.dot(est, gt) / denom

    diff = (est - gt)
    l2_diff = diff.norm()
    # rel_l2_diff = l2_diff / gt.norm().clamp_min(eps)
    return cos_sim, l2_diff

# wrapper function for BLOLayer
def BLOLayer(
    problem_list,
    parameters_list,
    variables_list,
    alpha: float = 100.0,
    dual_cutoff: float = 1e-3,
    slack_tol: float = 1e-8,
    compute_cos_sim: bool = False,
    eps: float = 1e-7,
):
    """
    Create an optimization layer that can be called like a CvxpyLayer:
        layer = BLOLayer(...);  y = layer(*param_tensors)

    Args:
        problem:   cvxpy.Problem with objective + constraints
        parameters: list[cp.Parameter]
        variables:  list[cp.Variable]
        alpha, dual_cutoff, slack_tol, compute_cos_sim: hyperparameters for BLOLayer

    Returns:
        A module with forward(*params, solver_args={}) -> tuple[tensor,...]
    """
    assert(len(problem_list) == len(parameters_list) == len(variables_list))
    N = len(problem_list)
    eq_funcs_list = []
    ineq_funcs_list = []  
    objective_expr_list = []  
    
    for i in range(N):
        problem = problem_list[i]
        parameters = parameters_list[i]
        variables = variables_list[i]
        
        # Extract objective expression (ensure minimization)
        obj = problem.objective
        if isinstance(obj, cp.Minimize):
            objective_expr = obj.expr
        elif isinstance(obj, cp.Maximize):
            objective_expr = -obj.expr  # convert to minimize
        else:
            objective_expr = getattr(obj, "expr", None)
            if objective_expr is None:
                raise ValueError("Unsupported objective type; expected Minimize/Maximize.")
        
        objective_expr_list.append(objective_expr)

        eq_funcs, ineq_funcs = [], []
        for c in problem.constraints:
            # Equality: g(x,θ) == 0  -> store g(x,θ)
            if isinstance(c, cp.constraints.zero.Equality):
                eq_funcs.append(c.expr)

            # Inequality: g(x,θ) <= 0 -> store g(x,θ)
            elif isinstance(c, cp.constraints.nonpos.Inequality):
                ineq_funcs.append(c.expr)

            else:
                # save for PSD or SOC constraints
                raise NotImplementedError(
                    f"Constraint type {type(c)} not supported in BLOLayer wrapper."
                )
        
        eq_funcs_list.append(eq_funcs)
        ineq_funcs_list.append(ineq_funcs)

    print(len(problem_list))
    print(len(ineq_funcs_list))
    print(len(parameters_list))
    print(len(variables_list))
    return _BLOLayer(
        objective_list=objective_expr_list,
        eq_functions_list=eq_funcs_list,
        ineq_functions_list=ineq_funcs_list,
        parameters_list=parameters_list,
        variables_list=variables_list,
        alpha=alpha,
        dual_cutoff=dual_cutoff,
        slack_tol=slack_tol,
        _compute_cos_sim=compute_cos_sim,
        eps=eps,
    )

class _BLOLayer(torch.nn.Module):
    """A differentiable convex optimization layer

    A BLOLayer solves a parametrized convex optimization problem given by a
    CVXPY problem. It solves the problem in its forward pass, and it computes
    the derivative of problem's solution map with respect to the parameters in
    its backward pass. The CVPXY problem must be a disciplined parametrized
    program.

    Example usage:
        ```
        import cvxpy as cp
        import torch
        from blolayers.torch import BLOLayer

        n, m = 2, 3
        x = cp.Variable(n)
        A = cp.Parameter((m, n))
        b = cp.Parameter(m)
        eq_constraints = [x = 0]
        ineq_constriants = [x >= 0
        objective = cp.Minimize(0.5 * cp.pnorm(A @ x - b, p=1))
        problem = cp.Problem(objective, eq_constraints + ineq_constraints)
        assert problem.is_dpp()

        blolayer = BLOLayer(problem, parameters=[A, b], variables=[x])
        A_tch = torch.randn(m, n, requires_grad=True)
        b_tch = torch.randn(m, requires_grad=True)

        # solve the problem
        solution, = blolayer(A_tch, b_tch)

        # compute the gradient of the sum of the solution with respect to A, b
        solution.sum().backward()
        ```
    """

    def __init__(self, objective_list, eq_functions_list, ineq_functions_list, parameters_list, variables_list, alpha, dual_cutoff, slack_tol, eps, _compute_cos_sim=False):
        """Construct a BLOLayer

        Args:
          objective: a CVXPY Objective object defining the objective of the
                     problem.
          eq_functions: a list of CVXPY Constraint objects defining the problem.
          ineq_functions: a list of CVXPY Constraint objects defining the problem.
          parameters: A list of CVXPY Parameters in the problem; the order
                      of the Parameters determines the order in which parameter
                      values must be supplied in the forward pass. Must include
                      every parameter involved in problem.
          variables: A list of CVXPY Variables in the problem; the order of the
                     Variables determines the order of the optimal variable
                     values returned from the forward pass.
        """
        super(_BLOLayer, self).__init__()
        
        num_batch = len(objective_list)
        assert(num_batch == len(eq_functions_list) == len(ineq_functions_list) == len(parameters_list) == len(variables_list))

        self.objective_list = objective_list
        self.eq_functions_list = eq_functions_list
        self.ineq_functions_list = ineq_functions_list
        self.param_order_list = parameters_list
        self.variables_list = variables_list
        self.alpha = alpha
        self.dual_cutoff = dual_cutoff
        self.slack_tol = float(slack_tol) 
        self._compute_cos_sim = _compute_cos_sim
        self.eps = eps
        
        self.eq_constraints_list = [[f == 0 for f in eq_functions] for eq_functions in eq_functions_list]
        self.ineq_constraints_list = [[g <= 0 for g in ineq_functions] for ineq_functions in ineq_functions_list]
        self.problem_list = [cp.Problem(cp.Minimize(objective_list[i]), self.eq_constraints_list[i] + self.ineq_constraints_list[i]) for i in range(num_batch)]

        self.dvar_params_list   = [[cp.Parameter(shape=v.shape) for v in variables] for variables in variables_list]
        self.eq_dual_params_list   = [[cp.Parameter(shape=f.shape) for f in eq_functions] for eq_functions in eq_functions_list]
        self.ineq_dual_params_list = [[cp.Parameter(shape=f.shape) for f in ineq_functions] for ineq_functions in ineq_functions_list]
        self.active_mask_params_list = [[cp.Parameter(shape=f.shape) for f in ineq_functions] for ineq_functions in ineq_functions_list]

        ######### original that is not dpp (parameter * parameter)
        vars_dvars_product_list = [cp.sum([cp.sum(cp.multiply(dv, v))
                                    for dv, v in zip(self.dvar_params_list[i], variables_list[i])]) for i in range(num_batch)]
        ineq_dual_product_list = [cp.sum([cp.sum(cp.multiply(lm, g))
                                    for lm, g in zip(self.ineq_dual_params_list[i], ineq_functions_list[i])]) for i in range(num_batch)]

        self.new_objective_list = [(1/self.alpha) * vars_dvars_product_list[i] + objective_list[i] + ineq_dual_product_list[i] for i in range(num_batch)]
        self.active_eq_constraints_list = [[
            cp.multiply(self.active_mask_params_list[i][j], ineq_functions[j]) == 0
            for j in range(len(ineq_functions))
        ] for i, ineq_functions in enumerate(ineq_functions_list)]
         
        self.perturbed_problem_list = [cp.Problem(cp.Minimize(self.new_objective_list[i]),
                                        self.eq_constraints_list[i] + self.active_eq_constraints_list[i]) for i in range(num_batch)]

        ######### new that is dpp (parameter*variable)
        # ineq_vals_list = [[cp.Variable(shape=g.shape, name=f"ineq_val_{k}_{i}")
        #     for k, g in enumerate(self.ineq_functions_list[i])] for i in range(num_batch)]
        
        # link_constraint_list = []
        # for i in range(num_batch):
        #     link_constraints = []
        #     for z, g in zip(ineq_vals_list[i], self.ineq_functions_list[i]):
        #         link_constraints.append(z == g)
        #     link_constraint_list.append(link_constraints)
            
        # vars_dvars_product_list = [cp.sum([cp.sum(cp.multiply(dv, v))
        #                             for dv, v in zip(self.dvar_params_list[i], variables_list[i])]) for i in range(num_batch)]
        
        # ineq_dual_product_list = [cp.sum([cp.sum(cp.multiply(lm, g))
        #                             for lm, g in zip(self.ineq_dual_params_list[i], ineq_vals_list[i])]) for i in range(num_batch)]

        # self.new_objective_list = [(1/self.alpha) * vars_dvars_product_list[i] + objective_list[i] + ineq_dual_product_list[i] for i in range(num_batch)]
        
        # self.active_eq_constraints_list = [[
        #     cp.multiply(self.active_mask_params_list[i][j], ineq_vals[j]) == 0
        #     for j in range(len(ineq_vals))
        # ] for i, ineq_vals in enumerate(ineq_vals_list)]
         
        # self.perturbed_problem_list = [cp.Problem(cp.Minimize(self.new_objective_list[i]),
        #                                 self.eq_constraints_list[i] + self.active_eq_constraints_list[i] + link_constraint_list[i]) for i in range(num_batch)]

        # print("new_objective DPP?", self.new_objective_list[0].is_dcp(dpp=True))
        # print("vars_dvars_product DPP?", vars_dvars_product_list[0].is_dcp(dpp=True))
        # print("active eq constraint DPP?", self.active_eq_constraints_list[0][0].is_dcp(dpp=True))
        # print("ineq_dual_product DPP?", ineq_dual_product_list[0].is_dcp(dpp=True))
        # # assert(1==0)

        
        
        self.phi_torch_list = []
        self.ineq_dual_term_torch_list = []
        self.eq_dual_term_torch_list = []
        for i in range(num_batch):
            objective = self.objective_list[i]
            eq_functions = self.eq_functions_list[i]
            ineq_functions = self.ineq_functions_list[i]
            variables = self.variables_list[i]

            ineq_dual_product = cp.sum([cp.sum(cp.multiply(du, f)) for du, f in zip(self.ineq_dual_params_list[i], ineq_functions)])
            if len(ineq_functions) > 0:
                self.ineq_dual_term_torch_list.append(TorchExpression(
                    ineq_dual_product,
                    provided_vars_list=[*variables, *self.param_order_list[i], *self.ineq_dual_params_list[i]]
                ).torch_expression)

            eq_dual_product = cp.sum([cp.sum(cp.multiply(du, f)) for du, f in zip(self.eq_dual_params_list[i], eq_functions)])
            if len(eq_functions) > 0:
                self.eq_dual_term_torch_list.append(TorchExpression(
                    eq_dual_product,
                    provided_vars_list=[*variables, *self.param_order_list[i], *self.eq_dual_params_list[i]]
                ).torch_expression)

            phi_expr = objective \
                + eq_dual_product \
                + ineq_dual_product
            phi_torch = TorchExpression(
                phi_expr,
                provided_vars_list=[*variables, *self.param_order_list[i], *self.eq_dual_params_list[i], *self.ineq_dual_params_list[i]]
            ).torch_expression
            
            self.phi_torch_list.append(phi_torch)
            
        self.forward_setup_time = None
        self.backward_setup_time = None
        self.forward_solve_time = None
        self.backward_solve_time = None
            

    def forward(self, *params, solver_args={}):
        """Solve problem (or a batch of problems) corresponding to `params`

        Args:
          params: a sequence of torch Tensors; the n-th Tensor specifies
                  the value for the n-th CVXPY Parameter. These Tensors
                  can be batched: if a Tensor has 3 dimensions, then its
                  first dimension is interpreted as the batch size. These
                  Tensors must all have the same dtype and device.
          solver_args: a dict of optional arguments, to send to `diffcp`. Keys
                       should be the names of keyword arguments.

        Returns:
          a list of optimal variable values, one for each CVXPY Variable
          supplied to the constructor.
        """
        if solver_args is None:
            solver_args = {}
        solver_args = {"warm_start": True, "solver": cp.SCS, **solver_args}
        
        info = {}
        f = _BLOLayerFn(
            blolayer=self,
            _compute_cos_sim=self._compute_cos_sim,
            info=info
        )
        sol = f(*params)
        self.info = info
        return sol

def _BLOLayerFn(
        blolayer,
        _compute_cos_sim,
        info):
    class _BLOLayerFnFn(torch.autograd.Function):
        @staticmethod
        def forward(ctx, *params):
            # infer dtype, device, and whether or not params are batched
            ctx.dtype = params[0].dtype
            ctx.device = params[0].device
            
            param_order = blolayer.param_order_list[0]

            ctx.batch_sizes = []
            for i, (p, q) in enumerate(zip(params, param_order)):
                # check dtype, device of params
                if p.dtype != ctx.dtype:
                    raise ValueError(
                        "Two or more parameters have different dtypes. "
                        "Expected parameter %d to have dtype %s but "
                        "got dtype %s." %
                        (i, str(ctx.dtype), str(p.dtype))
                    )
                if p.device != ctx.device:
                    raise ValueError(
                        "Two or more parameters are on different devices. "
                        "Expected parameter %d to be on device %s "
                        "but got device %s." %
                        (i, str(ctx.device), str(p.device))
                    )

                # check and extract the batch size for the parameter
                # 0 means there is no batch dimension for this parameter
                # and we assume the batch dimension is non-zero
                if p.ndimension() == q.ndim:
                    batch_size = 0
                elif p.ndimension() == q.ndim + 1:
                    batch_size = p.size(0)
                    if batch_size == 0:
                        raise ValueError(
                            "The batch dimension for parameter {} is zero "
                            "but should be non-zero.".format(i))
                else:
                    raise ValueError(
                        "Invalid parameter size passed in. Expected "
                        "parameter {} to have have {} or {} dimensions "
                        "but got {} dimensions".format(
                            i, q.ndim, q.ndim + 1, p.ndimension()))

                ctx.batch_sizes.append(batch_size)

                # validate the parameter shape
                p_shape = p.shape if batch_size == 0 else p.shape[1:]
                if not np.all(p_shape == param_order[i].shape):
                    raise ValueError(
                        "Inconsistent parameter shapes passed in. "
                        "Expected parameter {} to have non-batched shape of "
                        "{} but got {}.".format(
                                i,
                                q.shape,
                                p.shape))

            ctx.batch_sizes = np.array(ctx.batch_sizes)
            ctx.batch = np.any(ctx.batch_sizes > 0)

            if ctx.batch:
                nonzero_batch_sizes = ctx.batch_sizes[ctx.batch_sizes > 0]
                ctx.batch_size = nonzero_batch_sizes[0]
                if np.any(nonzero_batch_sizes != ctx.batch_size):
                    raise ValueError(
                        "Inconsistent batch sizes passed in. Expected "
                        "parameters to have no batch size or all the same "
                        "batch size but got sizes: {}.".format(
                            ctx.batch_sizes))
            else:
                ctx.batch_size = 1
            
            B = ctx.batch_size

            # convert to numpy arrays
            params_numpy = [to_numpy(p) for p in params]

            sol_numpy = [np.empty((B,) + v.shape, dtype=float) for v in blolayer.variables_list[0]]
            eq_dual = [np.empty((B,) + f.shape, dtype=float) for f in blolayer.eq_functions_list[0]]
            ineq_dual = [np.empty((B,) + g.shape, dtype=float) for g in blolayer.ineq_functions_list[0]]
            ineq_slack_residual = [np.empty((B,) + g.shape, dtype=float) for g in blolayer.ineq_functions_list[0]]
            
            def _solve_one_forward(i):
                # select per-slot params (same logic as before)
                if ctx.batch:
                    params_numpy_i = [p[i] if bs > 0 else p for p, bs in zip(params_numpy, ctx.batch_sizes)]
                else:
                    params_numpy_i = params_numpy

                # assign into per-slot cvxpy Parameters
                # k=0
                for p_val, param_obj in zip(params_numpy_i, blolayer.param_order_list[i]):
                    # param_obj is a cvxpy.Parameter (or maybe None if unused)
                    if param_obj is None:
                        continue
                    param_obj.value = p_val

                
                try:
                    # blolayer.problem_list[i].solve(solver=cp.GUROBI, ignore_dpp=True, warm_start=True, **{"Threads": n_threads, "OutputFlag": 0})
                    blolayer.problem_list[i].solve(solver=cp.SCS, warm_start=False, ignore_dpp=True, max_iters=2500, eps=blolayer.eps)
                except:
                    print("forward pass SCS failed, using OSQP")
                    blolayer.problem_list[i].solve(solver=cp.OSQP, warm_start=False, verbose=False)
                
                
                setup_time = blolayer.problem_list[i].compilation_time
                solve_time = blolayer.problem_list[i].solver_stats.solve_time
                
                # collect primal and dual outputs for this slot
                sol_vals = [v.value for v in blolayer.variables_list[i]]
                eq_vals = [c.dual_value for c in blolayer.eq_constraints_list[i]]
                ineq_vals = [c.dual_value for c in blolayer.ineq_constraints_list[i]]
                slack_vals = [np.maximum(-expr.value, 0.0) for expr in blolayer.ineq_functions_list[i]]

                return sol_vals, eq_vals, ineq_vals, slack_vals, setup_time, solve_time

            # run parallel solves while limiting BLAS/OpenMP threads to 1 (avoid oversubscription)
            with threadpool_limits(limits=1):
                pool = ThreadPool(processes=min(B, n_threads))
                try:
                    results = pool.map(_solve_one_forward, range(B))
                finally:
                    pool.close()
                    # pool.join()
                    
            avg_setup_time = 0.0
            avg_solve_time = 0.0

            for i, (sol_i, eq_i, ineq_i, slack_i, setup_time_i, solve_time_i) in enumerate(results):
                # convert to torch tensors and incorporate info_forward
                for v_id, v in enumerate(blolayer.variables_list[i]):
                    sol_numpy[v_id][i, ...] = sol_i[v_id]

                for c_id, c in enumerate(blolayer.eq_constraints_list[i]):
                    eq_dual[c_id][i, ...] = eq_i[c_id]

                for c_id, c in enumerate(blolayer.ineq_constraints_list[i]):
                    ineq_dual[c_id][i, ...] = ineq_i[c_id]

                for c_id, expr in enumerate(blolayer.ineq_functions_list[i]):
                    g_val = expr.value
                    s_val = -g_val
                    s_val = np.maximum(s_val, 0.0)
                    ineq_slack_residual[c_id][i, ...] = slack_i[c_id]
                    
                avg_setup_time += setup_time_i
                avg_solve_time += solve_time_i
            
            blolayer.forward_setup_time = avg_setup_time / B
            blolayer.forward_solve_time = avg_solve_time / B

            ctx.sol_numpy = sol_numpy
            ctx.eq_dual = eq_dual
            ctx.ineq_dual = ineq_dual
            ctx.params_numpy = params_numpy
            ctx.params = params
            ctx.slack = ineq_slack_residual
            ctx.blolayer = blolayer

            ctx._warm_vars_list = [[v.value.copy() for v in blolayer.variables_list[i]] for i in range(B)]
            ctx._warm_eq_duals_list = [[c.dual_value.copy() for c in blolayer.eq_constraints_list[i]] for i in range(B)]
            ctx._warm_ineq_duals_list = [[c.dual_value.copy() for c in blolayer.ineq_constraints_list[i]] for i in range(B)]
            ctx._warm_ineq_slack_residuals_list = [[c.value.copy() for c in blolayer.ineq_functions_list[i]] for i in range(B)]

            sol_torch = [to_torch(arr, ctx.dtype, ctx.device) for arr in sol_numpy]

            return tuple(sol_torch)

        @staticmethod
        def backward(ctx, *dvars):
            # convert to numpy arrays
            dvars_numpy = [to_numpy(dvar) for dvar in dvars]
            
            # temperature = 10
            # ineq_dual_tanh = [np.tanh(dual * temperature) for dual in ctx.ineq_dual]

            blolayer = ctx.blolayer
            sol_numpy = ctx.sol_numpy
            eq_dual = ctx.eq_dual
            ineq_dual = ctx.ineq_dual
            slack = ctx.slack
            y_dim = dvars_numpy[0].shape[1]
            if len(eq_dual) == 0:
                num_eq = 0
            else:
                num_eq = eq_dual[0].shape[1]
            B = ctx.batch_size

            params_numpy = ctx.params_numpy

            params_req, req_grad_mask = [], []
            for p in ctx.params:
                q = p.detach().clone()
                req_grad = bool(p.requires_grad)
                q.requires_grad_(req_grad)
                params_req.append(q)
                req_grad_mask.append(req_grad)

            if _compute_cos_sim:
                # compute ground truth gradient using cvxpylayer
                cvxpylayer_problem = cp.Problem(cp.Minimize(blolayer.objective),
                            constraints=blolayer.eq_constraints + blolayer.ineq_constraints)

                _cvx_layer = CvxpyLayer(cvxpylayer_problem, parameters=blolayer.param_order, variables=blolayer.variables)
                with torch.enable_grad():
                    sol_tensors = _cvx_layer(*params_req)

                if not isinstance(sol_tensors, (tuple, list)):
                    sol_tensors = (sol_tensors,)

                grad_outputs = [torch.zeros_like(out) if gv is None else gv for out, gv in zip(sol_tensors, dvars)]
                inputs_for_grad = tuple(q for q, need in zip(params_req, req_grad_mask) if need)

                ground_truth_grads = torch.autograd.grad(
                    outputs=tuple(sol_tensors),
                    inputs=inputs_for_grad,
                    grad_outputs=tuple(grad_outputs),
                    allow_unused=True,
                    retain_graph=False
                )

            new_sol_lagrangian = [np.empty_like(sol_numpy[k]) for k in range(len(blolayer.variables_list[0]))]
            new_eq_dual = [np.empty_like(eq_dual[k]) for k in range(len(blolayer.eq_constraints_list[0]))]

            new_active_dual = [np.empty((B,) + c.shape, dtype=float) for c in blolayer.active_eq_constraints_list[0]]
            
            def _solve_one_backward(i):
                for j, _ in enumerate(blolayer.param_order_list[i]):
                    blolayer.param_order_list[i][j].value = params_numpy[j][i]

                # ZIHAO ADDED
                for j, v in enumerate(blolayer.variables_list[i]):
                    blolayer.dvar_params_list[i][j].value = dvars_numpy[j][i]
                    v.value = ctx._warm_vars_list[i][j]
                for j, c in enumerate(blolayer.eq_constraints_list[i]):
                    if c.dual_value is None and ctx._warm_eq_duals_list[i][j] is not None:
                        c.dual_value = ctx._warm_eq_duals_list[i][j]
                for j, c in enumerate(blolayer.ineq_constraints_list[i]):
                    if c.dual_value is None and ctx._warm_ineq_duals_list[i][j] is not None:
                        c.dual_value = ctx._warm_ineq_duals_list[i][j]

                for j, _ in enumerate(blolayer.ineq_functions_list[i]):
                    # key for bilevel algorithm: identify the active constraints and add them to the equality constraints
                    lam = ineq_dual[j][i]
                    blolayer.ineq_dual_params_list[i][j].value = lam
                    
                    # active_mask_params[j].value = ((lam > dual_cutoff)).astype(np.float64)
                    _requires_active = (slack[j][i] <= blolayer.slack_tol).astype(np.float64)
                    blolayer.active_mask_params_list[i][j].value = _requires_active

                    # print(f"num active constraints: {active_mask_params[j].value.sum()}")
                    if _requires_active.sum() > y_dim - num_eq:
                        print(f"num active constraints: {_requires_active.sum()}")

                        k = int(y_dim - num_eq)
                        idx = np.argpartition(lam, -k)[-k:]
                        mask = np.zeros_like(lam, dtype=np.float64)
                        mask[idx] = 1.0
                        blolayer.active_mask_params_list[i][j].value = mask

                for j, _ in enumerate(blolayer.eq_functions_list[i]):
                    blolayer.eq_dual_params_list[i][j].value = eq_dual[j][i]

                # blolayer.perturbed_problem_list[i].solve(solver=cp.GUROBI, ignore_dpp=True, warm_start=True, **{"Threads": n_threads, "OutputFlag": 0})
                blolayer.perturbed_problem_list[i].solve(solver=cp.SCS, warm_start=True, ignore_dpp=True, max_iters=2500, eps=blolayer.eps)

                setup_time = blolayer.perturbed_problem_list[i].compilation_time
                solve_time = blolayer.perturbed_problem_list[i].solver_stats.solve_time

                st = blolayer.perturbed_problem_list[i].status
                sol_diffs = []
                try:
                    if st not in (cp.OPTIMAL, cp.OPTIMAL_INACCURATE):
                        raise RuntimeError(f"New bilevel problem status = {st}")
                    for j, v in enumerate(blolayer.variables_list[i]):
                        new_sol_lagrangian[j][i, ...] = v.value
                        sol_diff = np.linalg.norm(sol_numpy[j][i] - v.value)
                        sol_diffs.append(sol_diff)
                except:
                    print("backward pass SCS failed, using OSQP")
                    blolayer.perturbed_problem_list[i].solve(solver=cp.OSQP, eps_abs=1e-4, eps_rel=1e-4, warm_start=True, verbose=False)
                    for j, v in enumerate(blolayer.variables_list[i]):
                        new_sol_lagrangian[j][i, ...] = v.value
                        sol_diff = np.linalg.norm(sol_numpy[j][i] - v.value)
                        sol_diffs.append(sol_diff)
                return sol_diffs, setup_time, solve_time
            
            # run parallel solves while limiting BLAS/OpenMP threads to 1 (avoid oversubscription)
            with threadpool_limits(limits=1):
                pool = ThreadPool(processes=min(B, n_threads))
                try:
                    sol_diffs_list = pool.map(_solve_one_backward, range(B))
                finally:
                    pool.close()
                    # pool.join()
                    
            avg_setup_time = 0.0
            avg_solve_time = 0.0

            for i, (_, setup_time_i, solve_time_i) in enumerate(sol_diffs_list):
                # convert to torch tensors and incorporate info_forward
                avg_setup_time += setup_time_i
                avg_solve_time += solve_time_i
            
            blolayer.backward_setup_time = avg_setup_time / B
            blolayer.backward_solve_time = avg_solve_time / B


            for i in range(B):
                
                for c_id, c in enumerate(blolayer.eq_constraints_list[i]):
                    if c.dual_value.any() == None:
                        print(f"equality constraint {c_id} dual value is None")
                    new_eq_dual[c_id][i, ...] = c.dual_value
                for c_id, c in enumerate(blolayer.active_eq_constraints_list[i]):
                    if c.dual_value.any() == None:
                        print(f"active inequality constraint {c_id} dual value is None")
                    active_mask = np.array([a.value for a in blolayer.active_mask_params_list[i]])
                    new_active_dual[c_id][i, ...] = c.dual_value
            
            
            new_sol = [to_torch(v, ctx.dtype, ctx.device) for v in new_sol_lagrangian]
            new_ineq_dual_torch = [to_torch(v, ctx.dtype, ctx.device) for v in new_active_dual]
            new_eq_dual_torch = [to_torch(v, ctx.dtype, ctx.device) for v in new_eq_dual]


            params_req = []
            for p, need in zip(ctx.params, req_grad_mask):
                q = p.detach().clone()
                if need:
                    q.requires_grad_(True)
                    params_req.append(q)
                else:
                    params_req.append(q)
            if ctx.device != torch.device('cpu'):
                torch.set_default_device(torch.device(ctx.device))
            loss = 0.0
            
            with torch.enable_grad():
                for i in range(B):
                    vars_new_i = [v[i] for v in new_sol]
                    vars_old_i = [to_torch(sol_numpy[j][i], ctx.dtype, ctx.device) for j in range(len(blolayer.variables_list[i]))]
                    
                    params_i = slice_params_for_batch(params_req, ctx.batch_sizes, i)

                    new_eq_dual_i = [d[i] for d in new_eq_dual_torch]
                    new_ineq_dual_i = [d[i] for d in new_ineq_dual_torch]
                    old_eq_dual_i = [to_torch(eq_dual[j][i], ctx.dtype, ctx.device) for j in range(len(blolayer.eq_constraints_list[i]))]
                    old_ineq_dual_i = [to_torch(ineq_dual[j][i], ctx.dtype, ctx.device) for j in range(len(blolayer.ineq_functions_list[i]))]

                    if len(blolayer.ineq_dual_term_torch_list) > 0:
                        ineq_dual_term_new_i = blolayer.ineq_dual_term_torch_list[i](*vars_old_i, *params_i, *new_ineq_dual_i)
                        ineq_dual_term_old_i = blolayer.ineq_dual_term_torch_list[i](*vars_old_i, *params_i, *old_ineq_dual_i)
                    else:
                        ineq_dual_term_new_i = 0.0
                        ineq_dual_term_old_i = 0.0

                    if len(blolayer.eq_dual_term_torch_list) > 0:
                        eq_dual_term_new_i = blolayer.eq_dual_term_torch_list[i](*vars_old_i, *params_i, *new_eq_dual_i)
                        eq_dual_term_old_i = blolayer.eq_dual_term_torch_list[i](*vars_old_i, *params_i, *old_eq_dual_i)
                    else:
                        eq_dual_term_new_i = 0.0
                        eq_dual_term_old_i = 0.0

                    phi_new_i = blolayer.phi_torch_list[i](*vars_new_i, *params_i, *old_eq_dual_i, *old_ineq_dual_i)
                    phi_old_i = blolayer.phi_torch_list[i](*vars_old_i, *params_i, *old_eq_dual_i, *old_ineq_dual_i)
                    loss +=  phi_new_i + ineq_dual_term_new_i + eq_dual_term_new_i - phi_old_i - ineq_dual_term_old_i - eq_dual_term_old_i

                loss = blolayer.alpha * loss

            loss.backward()
            grads = [p.grad for p in params_req]

            if _compute_cos_sim:
                with torch.no_grad():
                    total_l2 = torch.sqrt(sum(
                        (g.detach().float() ** 2).sum()
                        for g in grads if g is not None
                    ))
                    total_inf = max(
                        (g.detach().float().abs().max() for g in grads if g is not None)
                    )

                cos_sim, l2_norm = _compare_grads(params_req, [p.grad for p in params_req if p.requires_grad], ground_truth_grads)
                print(f"cos_sim = {cos_sim:.6f}")
                wandb.log({
                    # "solution_distance": sol_dis,
                    "grad_l2": total_l2,
                    "grad_inf": total_inf,
                    "cos_sim": cos_sim,
                    "l2_norm": l2_norm,
                })

            return tuple(grads)

    return _BLOLayerFnFn.apply
