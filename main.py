import numpy as np
import torch
import time
import sys
import os
import argparse
import tqdm
# import qpth
from qpth.qp import QPFunction
import pickle
from cvxpylayers.torch import CvxpyLayer
import cvxpy as cp
import matplotlib.pyplot as plt
from torch.utils.tensorboard import SummaryWriter

import ffoqp

from AdamFFO import AdamFFO

from loss import *
from models import *
from data import *

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--method', type=str, default='ffoqp', help='ffoqp, ts, qpth')
    parser.add_argument('--epochs', type=int, default=20, help='number of epochs')
    parser.add_argument('--seed', type=int, default=0, help='random seed')
    parser.add_argument('--eps', type=float, default=0.1, help='lambda for ffoqp')
    parser.add_argument('--lr', type=float, default=0.01, help='learning rate')
    
    args = parser.parse_args()

    method = args.method
    seed = args.seed
    num_epochs = args.epochs
    eps = args.eps
    lamb = 1/(eps**2)
    learning_rate = args.lr

    # Set random seed for reproducibility
    torch.manual_seed(seed)
    np.random.seed(seed)

    input_dim   = 640
    output_dim  = 512
    n = output_dim
    num_samples = 2048
    batch_size = 32

    train_loader, test_loader = genData(input_dim, output_dim, num_samples, batch_size)

    model = MLP(input_dim, output_dim)
    if method == 'ffoqp':
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)
        # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-2)
    else:
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0)
        # optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-2)

    loss_fn = torch.nn.MSELoss()
    writer = SummaryWriter()

    # Setup the optimization problem
    Q = torch.eye(n)
    # p = y_pred
    G = torch.cat([torch.eye(n), -torch.eye(n), torch.ones(1,n)], dim=0)
    h = torch.cat([torch.zeros(n), torch.ones(n), torch.Tensor([3])], dim=0)
    A = torch.Tensor()
    b = torch.Tensor()

    deltas = [torch.zeros_like(parameter) for parameter in model.parameters()]
    gradients = [torch.zeros_like(parameter) for parameter in model.parameters()]
    eta = learning_rate
    D = eps**3

    # Constructing cvxpylayer instance
    n_ineq_constraints = 2 * n + 1
    Q_cp = cp.Parameter((n, n), PSD=True)
    q_cp = cp.Parameter(n)
    G_cp = cp.Parameter((n_ineq_constraints, n))
    h_cp = cp.Parameter(n_ineq_constraints)
    z_cp = cp.Variable(n)

    objective_fn = 0.5 * cp.sum_squares(Q_cp @ z_cp) + q_cp.T @ z_cp
    constraints = [G_cp @ z_cp <= h_cp]

    problem = cp.Problem(cp.Minimize(objective_fn), constraints)
    layer = CvxpyLayer(problem, parameters=[Q_cp, q_cp, G_cp, h_cp], variables=[z_cp])

    # Solver options
    # Note: the current version only works for the CVXPY solver
    # solver = QPSolvers.PDIPM_BATCHED

    ffoqp_layer = ffoqp.ffoqp(lamb=lamb, verbose=-1)
    qpth_layer = QPFunction(verbose=-1)

    s = 0
    ts_weight = 0
    norm_weight = 0
    for epoch in range(num_epochs):
        train_ts_loss_list, test_ts_loss_list = [], []
        train_df_loss_list, test_df_loss_list = [], []
        start_time = time.time()
        for i, (x, y) in enumerate(train_loader):
            y_pred = model(x)
            # y_pred.retain_grad()
            ts_loss = loss_fn(y_pred, y)
            # df_loss = df_loss_fn(y_pred, y)
            if method == 'ffoqp':
                # start_time = time.time()
                z = ffoqp_layer(Q, y_pred, G, h, A, b)
                loss = torch.mean(y * z) + ts_loss * ts_weight + torch.norm(z) * norm_weight
                # if i % 100 == 0:
                #     print('ffoqp time elapsed:', time.time() - start_time)
                # start_time = time.time()
                loss.backward()
                # if i % 100 == 0:
                #     print('ffoqp backward time elapsed:', time.time() - start_time)

                # s = torch.rand(1)
                # for i, parameter in enumerate(model.parameters()):
                #     deltas[i] = torch.clamp(deltas[i] - eta * parameter.grad, min=-D, max=D) # Clip delta
                #     parameter.grad = - s * deltas[i] - gradients[i]
                #     gradients[i] = deltas[i] * (1 - s)

                # for i, parameter in enumerate(model.parameters()):
                #     s = torch.randn(parameter.shape)
                #     parameter.grad += s * 0.01 - deltas[i]
                #     deltas[i] = s

                # y_grad = y_pred.grad
                # print(y_grad)
                # if y_grad is not None:
                #     y_grad = torch.mean(y_grad, dim=0, keepdim=True)
                #     D = learning_rate
                #     delta = delta - learning_rate * y_grad
                #     delta = torch.clamp(delta, min=-D, max=D)
                #     y_pred.grad = - delta.repeat(y_pred.shape[0], 1) / learning_rate
            elif method == 'ts':
                # z = torch.zeros(n)
                z = qpth_layer(Q, y_pred.detach(), G, h, A, b)
                loss = ts_loss
                loss.backward()
            elif method == 'qpth':
                z = qpth_layer(Q, y_pred, G, h, A, b)
                loss = torch.mean(y * z) + ts_loss * ts_weight + torch.norm(z) * norm_weight
                loss.backward()
            elif method == 'cvxpylayer':
                sol = layer(Q, y_pred, G, h)
                z = sol[0]
                loss = torch.mean(y * z) + ts_loss * ts_weight + torch.norm(z) * norm_weight
                loss.backward()

            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)
            optimizer.step()
            optimizer.zero_grad()

            df_loss = torch.mean(y * z) # + ts_loss

            train_ts_loss_list.append(ts_loss.item())
            train_df_loss_list.append(df_loss.item())

        print('time elapsed:', time.time() - start_time)

        for i, (x, y) in enumerate(test_loader):
            y_pred = model(x)
            ts_loss = loss_fn(y_pred, y)
            df_loss = df_loss_fn(y_pred, y)

            test_ts_loss_list.append(ts_loss.item())
            test_df_loss_list.append(df_loss.item())

        train_ts_loss = np.mean(train_ts_loss_list)
        train_df_loss = np.mean(train_df_loss_list)
        test_ts_loss = np.mean(test_ts_loss_list)
        test_df_loss = np.mean(test_df_loss_list)
        print("Epoch {}, Train TS Loss {}, Test TS Loss {}, Train DF Loss {}, Test DF Loss {}".format(epoch, train_ts_loss, test_ts_loss, train_df_loss, test_df_loss))

        writer.add_scalar('Loss/TS/train', train_ts_loss, epoch)
        writer.add_scalar('Loss/TS/test', test_ts_loss, epoch)
        writer.add_scalar('Loss/DF/train', train_df_loss, epoch)
        writer.add_scalar('Loss/DF/test', test_df_loss, epoch)

    writer.flush()

